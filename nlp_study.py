# -*- coding: utf-8 -*-
"""NLP_study.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/115IfpAqnI0GrlUAP6udFUomEbfe3yles

# 0.Importing libraries
"""

import numpy as np
import pandas as pd
import sklearn
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import tensorflow
import keras
import gensim
import warnings
warnings.filterwarnings('ignore')

#nltk.download()

!curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash

!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git

cd Mecab-ko-for-Google-Colab

!bash install_mecab-ko_on_colab_light_220429.sh

from konlpy.tag import Okt, Mecab

"""# 1.Warming up - Pandas & Numpy

##Pandas - Series
"""

sr = pd.Series([100, 200], index = ["pizza", "chicken"])
print("Print the Series :")
print('-' * 20)
print(sr)

print("Values of the series {}:" .format(sr.values))
print("Indices of the series {}:" .format(sr.index))

"""##Pandas - DataFrame"""

values = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
index = ['one', 'two', 'three']
columns = ['A', 'B', 'C']

df = pd.DataFrame(values, index = index, columns = columns)

print("Print the DataFrame : ")
print('-' * 20)
print(df)

print(df.values)
print(df.index)
print(df.columns)

df['A']

df.iloc[0]

"""##Numpy - array"""

vec = np.array(np.arange(1, 6))
mat = np.array([[10, 20, 30], [ 60, 70, 80]])

print(mat)

print(vec.ndim)
print(vec.shape)

print(mat.ndim)
print(mat.shape)

zero_mat = np.zeros((2, 3))
print(zero_mat)

one_mat = np.ones((2, 3))
print(one_mat)

same_value_mat = np.full((2, 3), 5)
print(same_value_mat)

eye_mat = np.eye(5)
print(eye_mat)

random_mat = np.random.random((2, 3))
print(random_mat)

reshape_mat = np.array(np.arange(30)).reshape((5, 6))
print(reshape_mat)

slicing_mat = reshape_mat[0, :]
print(slicing_mat)

indexing_mat = reshape_mat[[0, 1], [1, 2]]
print(indexing_mat)

mat1 = np.arange(1, 5).reshape((2, 2))
mat2 = np.arange(5, 9).reshape((2, 2))
mat3 = np.dot(mat1, mat2)
print(mat3)

"""##Matplotlib"""

plt.title('test')
plt.plot([1, 2, 3, 4], [2, 4, 8, 6])
plt.show()

plt.title('test')
plt.plot([1, 2, 3, 4], [2, 4, 8, 6])
plt.xlabel('hours')
plt.ylabel('score')
plt.show()

plt.title('test')
plt.plot([1, 2, 3, 4], [2, 4, 8, 6])
plt.plot([1.5, 2.5, 3.5, 4.5], [3, 5, 8, 10])
plt.xlabel('hours')
plt.ylabel('score')
plt.legend(['A student', 'B student'])
plt.show()

"""# 2.Text preprocessing

##2-1.Tokenization

Terminology
*    Corpus: Natural Language Data
*    Tokenization: Deviding Data into units called "Token"
*    Token: Base unit for tokenization which can be from a charactor to words

Required Libraries or Installs for this chapter
1. For word tokenization
*    from nltk.tokenize import word_tokenize, WordPunctTokenizer
*    from tensorflow.keras.preprocessing.text import text_to_word_sequence
*    from nltk.tokenize import TreebankWordTokenizer
2. For sentence tokenization
*    from nltk.tokenize import sent_tokenize
3. For Korean sentence tokenization
*    pip install kss
*    import kss
4. For Part-Of-Speech tagging
*    from nltk.tag import pos_tag
5. For overall Korean NLP preprocessing
*    from konlpy.tag import Okt
*    from konlpy.tag import Kkma

###Word Tokenization
We can't simply tokenize one or more sentences because there are punctuations and symbols.

 And someitmes they have meaning but somtimes they don't.
"""

#Three ways for word tokenization
from nltk.tokenize import word_tokenize, WordPunctTokenizer
from tensorflow.keras.preprocessing.text import text_to_word_sequence

#You will need this cell to prevent an error
nltk.download('punkt')

"""Example for dealing with apostrophe"""

text = "Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pasty shop."

print('Tokenize word1 : ',
      word_tokenize(text))

"""word_tokenize's result
* Don't -> Do, n't
* Jone's -> Jone, 's
"""

print('Tokenize word2 : ',
      WordPunctTokenizer().tokenize(text))

"""WordPunctTokenizer's result
* Don't -> Don, ', t
* Jone's -> Jone, ', s
"""

print('Tokenize word3 : ',
      text_to_word_sequence(text))

"""text_to_word_sequence's result
* Don't -> don't
* Jone's -> jone's

On the other hand, Tokenization is not simply about separating words.

Since we will face hyphenated words and even punctuations, and we need meaning from them, sometimes we should keep them.
"""

from nltk.tokenize import TreebankWordTokenizer

tokenizer = TreebankWordTokenizer()

text = "Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own."

print("Treebank word tokenizer : ", tokenizer.tokenize(text))

"""TreebankWordTokenizer's result
* home-bird -> home-bird
* doesn't -> does, n't

So, now we can conclude that every function has its own rules.

And we should be capable of choosing the best one for the purpose.

###Sentence Tokenization
Sometimes, punctuations don't work as its sentence's boundary.

Let's check if our nltk's sent_tokenizer recognize boundary one.
"""

from nltk.tokenize import sent_tokenize

text = "His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near."

print("Sentence tokenization1 : ", sent_tokenize(text))

"""It seems it works pretty well.

Let's check with other corpus.
"""

text = "I am actively looking for Ph.D. students. and you are a Ph.D student."
print("Sentence tokenization2 : ", sent_tokenize(text))

"""In the text, there is 'Ph.D' where the dot is not boundary.

Still it separated sentences perfectly!

This time, let's look at the sentence tokenizer for Korean Language
"""

!pip install kss

import kss

text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'

print('Korean sentence tokenization : ', kss.split_sentences(text))

"""It works well! The example sentences make me a little scared though...

###POS tagging
Since Korean language is agglutinative language and has difficult rules for spacing, we need to analyse POS, Part-Of-Speech.

You can do the POS tagging with nltk.tag's pos_tag function.
"""

from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
nltk.download('averaged_perceptron_tagger')

text = "I am actively looking for Ph.D. students. and you are a Ph.D. student."
tokenized_sentence = word_tokenize(text)

print('Word tokenization :', tokenized_sentence)
print('POS tagging : ',pos_tag(tokenized_sentence))

"""For Korean language, you can use konlpy.tag's Okt or Kkma"""

from konlpy.tag import Okt
from konlpy.tag import Kkma

okt = Okt()
kkma = Kkma()

print("OKT morpheme analysis : ", okt.morphs("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print("OKT POS tagging : ", okt.pos("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print("OKT extracting Noun : ", okt.nouns("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))

print('꼬꼬마 morpheme analysis : ', kkma.morphs("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print('꼬꼬마 POS tagging : ', kkma.pos("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print('꼬꼬마 extracting Noun : ', kkma.nouns("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))

"""###Sum up
* We need to preprocess corpus and one of its step is tokenization.
* Tokenization depends on the characteristics of the language and the purpose.
* You need to be able to choose the best way to preserve as much imformation as possible for your goal

##2-2.Cleaning and Normalization

Terminology
* Cleaning: Getting rid of noise from corpus
* Normalization: Converting different words with the same meaning into the same word

We conduct cleaning and normalization before or after tokenization.

The goal is to eliminate noise and stopwords which are not meaningful for the purpose.

The next cell shows an example of dealing with corpus using Regular Expression.
"""

import re
text = "I was wondering if anyone out there could enlighten me on this car."
shortword = re.compile(r'\w*\b\w{1,2}\b')
print(shortword.sub('', text))

"""As we can see in the cell above, we can delete words that are too short using Regular Expression.

##2-3.Stemming and Lemmatization

Terminology
* Stemming: extracting stem from a word
* Stem: a part of the word that has essential meaning
* Affix: a part of the word that has additional meaning
* Lemma: dictionary form of the word

In this part, we ara going to look into stemming and lemmatization.

Those are used for normalization.

1. Lemmatization
"""

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']

print('Before lemmatization : ', words)
print('After lemmatization : ',[lemmatizer.lemmatize(word) for word in words])

lemmatizer.lemmatize('dies', 'v')

lemmatizer.lemmatize('watched', 'v')

lemmatizer.lemmatize('has', 'v')

"""2. Stemming

Stemming is not a sophisticated process.

Because its approach is based on simple rules.
"""

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
nltk.download('punkt')

stemmer = PorterStemmer()

sentence = "This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes."
tokenized_sentence = word_tokenize(sentence)

print("Before stemming : ", tokenized_sentence)
print("After stemming : ", [stemmer.stem(word) for word in tokenized_sentence])

words = ['formalize', 'allowance', 'electricical']

print("Before stemming : ", words)
print("After stemming : ", [stemmer.stem(word) for word in words])

"""It's not working correctly because it simply follows the rules."""

from nltk.stem import PorterStemmer
from nltk.stem import LancasterStemmer

porter_stemmer = PorterStemmer()
lancaster_stemmer = LancasterStemmer()

words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']
print("Before stemming : ", words)
print("After Porter Stemming : ", [porter_stemmer.stem(word) for word in words])
print("After Lancaster Stemming : ", [lancaster_stemmer.stem(word) for word in words])

"""Each stemmer has its own pros and cones.

So we should decide which one to employ based on the situation we are facing.

And we can apply lemmatization and stemming to korean corpus.

But make sure you are considering Korean grammer rules and your goal.

##2-4.Stopword

Stopwords are words that frequently appear but are not meaningful for the NLP analysis.
"""

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from konlpy.tag import Okt

"""We can import preset stopword list."""

nltk.download('stopwords')
stop_words_list = stopwords.words('english')
print('Number of stopword : ', len(stop_words_list))
print('Print 10 stopwords : ', stop_words_list[:10])

"""And also delete stopwards from your corpus with it."""

example = "Family is not an important thing. It's everything."
stop_words = set(stopwords.words('english'))
word_tokens = word_tokenize(example)
result = []
for word in word_tokens:
  if word not in stop_words:
    result.append(word)
print("Before eliminating stopword : ",word_tokens)
print("After eliminating stopword : ", result)

"""Like the following example, you can make a set of stopwords and eliminate them from your corpus."""

#Korean
okt = Okt()

example = "고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지."
stop_words = "를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는"

stop_words = set(stop_words.split(' '))
word_tokens = okt.morphs(example)

result = [word for word in word_tokens if not word in stop_words]

print('Before eliminating stopword : ', word_tokens)
print('After eliminating stopword : ', result)

"""##2-5.Regular Expression

I will skip this part...

Regular Expression is useful for cleaning and normalization.

But you should learn it by yourself.
"""

import re

r = re.compile("a.c")
r.search("kkk")

r.search("abc")

r = re.compile("ab?c")
r.search("abbc")

r.search("abc")

r.search("ac")

r = re.compile("ab*c")
r.search("a")

r.search("ac")

r.search("abc")

r.search("abbbc")

r = re.compile("ab+c")
r.search("ac")

r.search("abc")

r.search("abbbbc")

r = re.compile("^ab")
r.search("bbc")
r.search("zab")

r.search("abz")

r = re.compile("ab{2}c")
r.search("ac")
r.search("abc")
r.search("abbbbc")

r.search("abbc")

r = re.compile("ab{2,8}c")
r.search("ac")
r.search("abc")
r.search("abbbbbbbbbc")

r.search("abbc")

r.search("abbbbbbbbc")

r = re.compile("a{2,}bc")
r.search("bc")
r.search("aa")

r.search("aabc")

r.search("aaaaaaaabc")

r = re.compile("[abc]") #[abc] equals to [a-c]
r.search("zzzz")

r.search("a")

r.search("aaaa")

r.search("zbaac")

r = re.compile("[a-z]")
r.search("AAA")
r.search("111")

r.search("aBC")

r = re.compile("[^abc]")
r.search("a")
r.search("ab")
r.search("b")

r.search("d")

r.search("1")

r = re.compile("ab.")
r.match("kkkabc")

r.match("abckkk")

text = "사과 딸기 수박 메론 바나나"
re.split(" ", text)

text = """사과
딸기
수박
메론
바나나"""

re.split("\n", text)

text = "사과+딸기+수박+메론+바나나"

re.split("\+", text)

text = """이름 : 김철수
전화번호 : 010 - 1234 - 1234
나이 : 30
성별 : 남"""

re.findall("\d+", text)

re.findall("\d+", "문자열입니다.")

text = "Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern."

preprocessed_text = re.sub('[^a-zA-Z]', ' ', text)
print(preprocessed_text)

text = """100 John    PROF
101 James   STUD
102 Mac   STUD"""

re.split('\s+', text)

re.findall('\d+', text)

re.findall('[A-Z]', text)

re.findall('[A-Z]{4}', text)

re.findall('[A-Z][a-z]+', text)

from nltk.tokenize import RegexpTokenizer

text = "Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop"

tokenizer1 = RegexpTokenizer("[\w]+")
tokenizer2 = RegexpTokenizer("\s+", gaps = True)

print(tokenizer1.tokenize(text))
print(tokenizer2.tokenize(text))

"""##2-6.Integer Encoding

Computers can calculate numbers better than characters.

So in this part, we are going to map tokenized words to integers for the better computing.

###1)Integer Encoding

We can assign a number based on the word's frequency rank in the corpus.

Here's an example.
"""

from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

raw_text = "A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain."

nltk.download('punkt')
sentences = sent_tokenize(raw_text)
print(sentences)

nltk.download('stopwords')
vocab = {}
preprocessed_sentences = []
stop_words = set(stopwords.words('english'))

for sentence in sentences:
  tokenized_sentence = word_tokenize(sentence)
  result = []

  for word in tokenized_sentence:
    word = word.lower()

    if word not in stop_words:

      if len(word) > 2:
        result.append(word)

        if word not in vocab:
          vocab[word] = 0
        vocab[word] += 1
  preprocessed_sentences.append(result)
print(preprocessed_sentences)

"""Tokenization, cleaning, normalization and counting.

For counting, you can view the importance of normalization.
"""

print("Word set :", vocab)

print(vocab["barber"])

"""And you can sort the dictionary in descending order."""

vocab_sorted = sorted(vocab.items(), key = lambda x : x[1], reverse = True)
print(vocab_sorted)

"""Assigning numebrs by order."""

word_to_index = {}
i = 0
for (word, frequency) in vocab_sorted:
  if frequency > 1:
    i = i + 1
    word_to_index[word] = i
print(word_to_index)

"""You can choose how many tokens you are going to use by the frequency ranking."""

vocab_size = 5

words_frequency = [word for word, index in word_to_index.items() if index >= vocab_size + 1]

for w in words_frequency:
  del word_to_index[w]
print(word_to_index)

"""If you decide the rank, the problem now is that there must be some tokens that are out of vocabulary; we call them OOV."""

word_to_index['OOV'] = len(word_to_index) + 1
print(word_to_index)

"""So the following cell shows how to treat OOV; it assigned them a new number."""

encoded_sentences = []
for sentence in preprocessed_sentences:
  encoded_sentence = []
  for word in sentence:
    try:
      encoded_sentence.append(word_to_index[word])
    except KeyError:
      encoded_sentence.append(word_to_index['OOV'])
  encoded_sentences.append(encoded_sentence)
print(encoded_sentences)

"""###2)Using Counter

Actually, we have simpler method.

Let's check it out!
"""

from collections import Counter

print(preprocessed_sentences)

#words = np.hstack(preprocessed_sentences)
all_words_list = sum(preprocessed_sentences, [])
print(all_words_list)

vocab = Counter(all_words_list)
print(vocab)

print(vocab["barber"])

vocab_size = 5
vocab = vocab.most_common(vocab_size)
vocab

"""As you see, Counter method automatically counts the number of same tokens."""

word_to_index = {}
i = 0
for (word, frequency) in vocab:
  i = i + 1
  word_to_index[word] = i
print(word_to_index)

"""###3)Using FreqDist of NLTK

And just like Counter, we have another method.
"""

from nltk import FreqDist
import numpy as np

vocab = FreqDist(np.hstack(preprocessed_sentences))

print(vocab["barber"])

vocab_size = 5
vocab = vocab.most_common(vocab_size)
print(vocab)

word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}
print(word_to_index)

"""**Understanding enumerate**

Just like the cell above, we can use enumerate.

It returns input's value and its key.
"""

test_input = ['a', 'b', 'c', 'd', 'e']
for index, value in enumerate(test_input):
  print("value : {}, index : {}".format(value, index))

"""###Text preprocessing with Keras

A whole workflow so far with keras
"""

from tensorflow.keras.preprocessing.text import Tokenizer

preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]

tokenizer = Tokenizer()

tokenizer.fit_on_texts(preprocessed_sentences)

print(tokenizer.word_index)

print(tokenizer.word_counts)

print(tokenizer.texts_to_sequences(preprocessed_sentences))

vocab_size = 5
tokenizer = Tokenizer(num_words = vocab_size + 1)
tokenizer.fit_on_texts(preprocessed_sentences)

print(tokenizer.word_index)

print(tokenizer.word_counts)

print(tokenizer.texts_to_sequences(preprocessed_sentences))

tokenizer = Tokenizer()
tokenizer.fit_on_texts(preprocessed_sentences)

"""Here's another example of the whole precess so far.

This example is different because it assigns 1 on OOV.
"""

vocab_size = 5
words_frequency = [word for word, index in tokenizer.word_index.items() if index >= vocab_size + 1]

for word in words_frequency:
  del tokenizer.word_index[word]
  del tokenizer.word_counts[word]

print(tokenizer.word_index)
print(tokenizer.word_counts)
print(tokenizer.texts_to_sequences(preprocessed_sentences))

voacb_size = 5
tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')
tokenizer.fit_on_texts(preprocessed_sentences)

print('Number of OOV\'s index : {}'.format(tokenizer.word_index['OOV']))

print(tokenizer.texts_to_sequences(preprocessed_sentences))

"""##2-7.Padding

After tokenization, if the length of the sentence or the passage,

 you can switch the tokens into vectors comprised of numbers

 so that you can improve the speed of calculation.

###1.Padding with Numpy
"""

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer

preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]

tokenizer = Tokenizer()
tokenizer.fit_on_texts(preprocessed_sentences)
encoded = tokenizer.texts_to_sequences(preprocessed_sentences)
print(encoded)

max_len = max(len(item) for item in encoded)
print("Max length :", max_len)

for sentence in encoded:
  while len(sentence) < max_len:
    sentence.append(0)
padded_np = np.array(encoded)
padded_np

"""As you see, we appended 0s after all the existing elements.

###2.Padding with Keras preprocessing
"""

from tensorflow.keras.preprocessing.sequence import pad_sequences

encoded = tokenizer.texts_to_sequences(preprocessed_sentences)
print(encoded)

padded = pad_sequences(encoded)
padded

"""However, in Keras, you can put 0s before the existing elements."""

padded = pad_sequences(encoded, padding = 'post')
padded

"""Of course you can put 0s after them."""

(padded == padded_np).all()

"""And with Keras, you can set the maximum length."""

padded = pad_sequences(encoded, padding = 'post', maxlen = 5)
padded

padded = pad_sequences(encoded, padding = 'post', truncating = 'post', maxlen = 5)
padded

"""We can set the dummy number as whatever we want instead of 0."""

last_value = len(tokenizer.word_index) + 1
print(last_value)

padded = pad_sequences(encoded, padding = 'post', value = last_value)
padded

"""##2-8.One-Hot Encoding

One-Hot Encoding is a good way to improve your computer's calculation speed.

In One-Hot Encoding, each column represents a certain token.

And each row has the only one 1 value which represents which token the row was.

But it has side effect in which we need more memory.

Let's look into the following example.

###with konlpy
"""

from konlpy.tag import Okt

okt = Okt()
tokens = okt.morphs("나는 자연어 처리를 배운다")
print(tokens)

word_to_index = {word : index for index, word in enumerate(tokens)}
print("Word set :", word_to_index)

"""Here, we define a function and put the only 1 in the row."""

def one_hot_encoding(word, word_to_index):
  one_hot_vector = [0] * (len(word_to_index))
  index = word_to_index[word]
  one_hot_vector[index] = 1
  return one_hot_vector

one_hot_encoding("자연어", word_to_index)

"""###with Keras"""

text = "나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야"

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical

text = "나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야"

tokenizer = Tokenizer()
tokenizer.fit_on_texts([text])
print("Word set :", tokenizer.word_index)

sub_text = "점심 먹으러 갈래 메뉴는 햄버거 최고야"
encoded = tokenizer.texts_to_sequences([sub_text])[0]
print(encoded)

"""Keras is rather simpler than the previous way."""

one_hot = to_categorical(encoded)
print(one_hot)

"""##2-9.Splitting Data

To improve your model's accuracy, you will need to split your data set.

It's called splitting.

Normally, when we are splitting data set, we split it into 3 sets.

Train set, validation set, test set.

You train your model with train set and use the validation set to prevent your model from overfitting.

For the last step, you will use test set to evaluate your model.
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

"""###1.Splitting into X and y from the original data

Zip function
"""

#with zip function
X, y = zip(['a', 1], ['b', 2], ['c', 3])
print("X data :", X)
print("y data :", y)

sequences = [['a', 1], ['b', 2], ['c', 3]]
X, y = zip(*sequences)
print("X data :", X)
print("y data :", y)

"""With DataFrame"""

#with DataFrame

values = [['당신에게 드리는 마지막 혜택!', 1],
['내일 뵐 수 있을지 확인 부탁드...', 0],
['도연씨. 잘 지내시죠? 오랜만입...', 0],
['(광고) AI로 주가를 예측할 수 있다!', 1]]
columns = ['메일 본문', '스팸 메일 유무']

df = pd.DataFrame(values, columns = columns)
df

X = df['메일 본문']
y = df['스팸 메일 유무']

print("X data :", X.to_list())
print("y data :", y.to_list())

"""With Numpy"""

#with Numpy
np_array = np.arange(0, 16).reshape((4, 4))
print("Whole data :")
print(np_array)

X = np_array[:, :3]
y = np_array[:, 3]

print("X data :")
print(X)
print("y data :", y)

"""###2.Extracting test data

With sklearn
"""

#with scikit-learn

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1234)

X, y = np.arange(10).reshape((5, 2)), range(5)

print("Whole X data :")
print(X)
print("Whole y data :")
print(list(y))

"""All you need to do is using the train_test_split function."""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1234)

print("X train data :")
print(X_train)
print("X test data :")
print(X_test)

print("y train data :")
print(y_train)
print("y test data :")
print(y_test)

"""You can set different random_state value."""

#different value for random_state

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)

print("y train data :")
print(y_train)
print("y test data :")
print(y_test)

# back to 1234 for random_state value
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)

print('y 훈련 데이터 :')
print(y_train)
print('y 테스트 데이터 :')
print(y_test)

"""You can also manually split your data."""

#manual extracting from here
X, y = np.arange(0, 24).reshape((12, 2)), range(12)

print("Whole X data :")
print(X)
print("Whole y data :")
print(list(y))

"""\*Warning!

you should substract the number for train set length from the whole length

If you set the number of test set length by multiplying 0.2, there must be an omission

because in python it will automatically switch floating point number to the integer type
"""

num_of_train = int(len(X) * 0.8)
num_of_test = int(len(X) - num_of_train)
print("Size of train data :", num_of_train)
print("Size of test data :", num_of_test)

X_test = X[num_of_train:]
y_test = y[num_of_train:]
X_train = X[:num_of_train]
y_train = y[:num_of_train]

print("X test data :")
print(X_test)
print("y test data :")
print(list(y_test))

"""##2-10.Text Preprocessing Tools for Korean Text

###1.PyKoSpacing

This package is for correcting inappropriately spaced sentences.
"""

pip install git+https://github.com/haven-jeon/PyKoSpacing.git

sent = '김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.'

new_sent = sent.replace(" ", "")
print(new_sent)

from pykospacing import Spacing
spacing = Spacing()
kospacing_sent = spacing(new_sent)

print(sent)
print(kospacing_sent)

"""###2.Py-Hanspell

*   Skip running the following cells
*   Because currently Py-hanspell doesn't work. (03/03/2024)

This one is for grammar check and spacing.




"""

#pip install git+https://github.com/ssut/py-hanspell.git

#from hanspell import spell_checker

#sent = "맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지 "
#spelled_sent = spell_checker.check(sent)

#hanspell_sent = spelled_sent.checked
#print(hanspell_sent)

#spelled_sent = spell_checker.check(new_sent)

#hanspell_sent = spelled_sent.checked
#print(hanspell_sent)
#print(kospacing_sent)

"""###3.Tokenization with SOYNLP

This package is based on unsupervised learning.

Since it's based on the unsupervised learning, it is very flexible for new words.

But you need to train the model of it first.
"""

pip install soynlp

from konlpy.tag import Okt
tokenizer = Okt()
print(tokenizer.morphs("에이비식스 이대휘 1월 최애돌 기부 요정"))

import urllib.request
from soynlp import DoublespaceLineCorpus
from soynlp.word import WordExtractor

urllib.request.urlretrieve("https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt", filename="2016-10-20.txt")

corpus = DoublespaceLineCorpus("2016-10-20.txt")
len(corpus)

i = 0
for document in corpus:
  if len(document) > 0:
    print(document)
    i = i + 1
  if i == 3:
    break

word_extractor = WordExtractor()
word_extractor.train(corpus)
word_score_table = word_extractor.extract()

"""SOYNLP is based on cohesion probability.

Cohesion probability shows the probability of the next character based on the previous substring.

This is specifically useful to distinguish a new korean word.
"""

word_score_table["반포한"].cohesion_forward

word_score_table["반포한강"].cohesion_forward

word_score_table["반포한강공"].cohesion_forward

word_score_table["반포한강공원"].cohesion_forward

word_score_table["반포한강공원에"].cohesion_forward

"""SOYNLP is also based on branching entropy.

Branching entropy is uncertainty of the next character.

If I spell B, you never know what is the word I'm thinking of.

Now, I spell Bana, then you will see I'm thinking of Banana.

This is what branching enptropy is like.
"""

word_score_table["디스"].right_branching_entropy

word_score_table["디스플"].right_branching_entropy

word_score_table["디스플레"].right_branching_entropy

word_score_table["디스플레이"].right_branching_entropy

"""Since Korean is agglutinative language, SOYNLP's LTokenizer is very useful.

It can seperate a word into left one(stem) and right one(affix) based on the score.
"""

from soynlp.tokenizer import LTokenizer

scores = {word : score.cohesion_forward for word, score in word_score_table.items()}
l_tokenizer = LTokenizer(scores = scores)
l_tokenizer.tokenize("국제사회와 우리의 노력들로 범죄를 척결하자", flatten = False)

"""It can also make a space between words that are sticked together."""

from soynlp.tokenizer import MaxScoreTokenizer

maxscore_tokenizer = MaxScoreTokenizer(scores = scores)
maxscore_tokenizer.tokenize("국제사회와우리의노력들로범죄를척결하자")

"""###4.Cleaning repetitive letters with SOYNLP

This is as you see!

You can even do the cleaning with SOYNLP!
"""

from soynlp.normalizer import *

print(emoticon_normalize('앜ㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ', num_repeats = 2))
print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠ', num_repeats=2))
print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠ', num_repeats=2))
print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠㅠㅠㅠ', num_repeats=2))

print(repeat_normalize('와하하하하하하하하하핫', num_repeats=2))
print(repeat_normalize('와하하하하하하하핫', num_repeats=2))
print(repeat_normalize('와하하하하하핫', num_repeats=2))

"""###5.Customized KoNLPy

You can also make your own rules for morphology!
"""

pip install customized_konlpy

from ckonlpy.tag import Twitter
twitter = Twitter()
twitter.morphs('은경이는 사무실로 갔습니다.')

twitter.add_dictionary('은경이', 'Noun')

twitter.morphs('은경이는 사무실로 갔습니다.')

"""#3.Language Model

##4. Count based word Representation

###4-2. Bag of Words
"""

from konlpy.tag import Okt

okt = Okt()

def build_bag_of_words(document):
  document = document.replace('.', '')
  tokenized_document = okt.morphs(document)

  word_to_index = {}
  bow = []

  for word in tokenized_document:
    if word not in word_to_index.keys():
      word_to_index[word] = len(word_to_index)
      bow.insert(len(word_to_index) - 1, 1)
    else:
      index = word_to_index.get(word)
      bow[index] = bow[index] + 1

  return word_to_index, bow

doc1 = "정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다."
vocab, bow = build_bag_of_words(doc1)
print("Vocabulary :", vocab)
print("Bag of words vector :", bow)

doc2 = '소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.'

vocab, bow = build_bag_of_words(doc2)
print("Vocabulary :", vocab)
print("Bag of words vector :", bow)

doc3 = doc1 + ' ' + doc2
vocab, bow = build_bag_of_words(doc3)
print("Vocabulary :", vocab)
print("Bag of words vector :", bow)

"""Making BoW with CountVectorizer"""

from sklearn.feature_extraction.text import CountVectorizer

corpus = ['you know I want your love. because I love you.']
vector = CountVectorizer()

print("Bag of words vector :", vector.fit_transform(corpus).toarray())

print("Vocabulary :", vector.vocabulary_)

from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords

text = ["Family is not an important thing. It's everything."]
vect = CountVectorizer(stop_words = ["the", "a", "an", "is", "not"])
print("Bag of words vector :", vect.fit_transform(text).toarray())
print("Vocabulary :", vect.vocabulary_)

text = ["Family is not an important thing. It's everything."]
vect = CountVectorizer(stop_words = "english")
print("Bag of words vector :", vect.fit_transform(text).toarray())
print("Vocabulary :", vect.vocabulary_)

nltk.download('stopwords')
text = ["Family is not an important thing. It's everything."]
stop_words = stopwords.words("english")
vect = CountVectorizer(stop_words = stop_words)
print("Bag of words vector :", vect.fit_transform(text).toarray())
print("Vocabulary :", vect.vocabulary_)

"""##4-4. TF-IDF(Term Frequency-Inverse Document Frequency)

Implementation with Python
"""

import pandas as pd
from math import log

docs = [
  '먹고 싶은 사과',
  '먹고 싶은 바나나',
  '길고 노란 바나나 바나나',
  '저는 과일이 좋아요'
]
vocab = list(set(w for doc in docs for w in doc.split()))
vocab.sort()

N = len(docs)

def tf(t, d):
  return d.count(t)

def idf(t):
  df = 0
  for doc in docs:
    df += t in doc
  return log(N / (df + 1))

def tfidf(t, d):
  return tf(t, d)* idf(t)

result = []

for i in range(N):
  result.append([])
  d = docs[i]
  for j in range(len(vocab)):
    t = vocab[j]
    result[-1].append(tf(t, d))

tf_ = pd.DataFrame(result, columns = vocab)
tf_

result = []
for j in range(len(vocab)):
  t = vocab[j]
  result.append(idf(t))

idf_ = pd.DataFrame(result, index = vocab, columns = ["IDF"])
idf_

result = []
for i in range(N):
  result.append([])
  d = docs[i]
  for j in range(len(vocab)):
    t = vocab[j]
    result[-1].append(tfidf(t, d))

tfdif_ = pd.DataFrame(result, columns = vocab)
tfdif_

"""Implementation with sklearn"""

from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    'you know I want your love',
    'I like you',
    'what should I do ',
]

vector = CountVectorizer()

print(vector.fit_transform(corpus).toarray())

print(vector.vocabulary_)

from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
    'you know I want your love',
    'I like you',
    'what should I do ',
]

tfidfv = TfidfVectorizer().fit(corpus)
print(tfidfv.transform(corpus).toarray())
print(tfidfv.vocabulary_)

