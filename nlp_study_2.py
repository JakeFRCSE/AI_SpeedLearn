# -*- coding: utf-8 -*-
"""NLP_study_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z8XKJfg9LXmtPTMLMU_AKc8IbJYsPBuG

#9.Word Embedding

##9-4 Skip-Gram with Negative Smapling
"""

import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from sklearn.datasets import fetch_20newsgroups
from tensorflow.keras.preprocessing.text import Tokenizer

dataset = fetch_20newsgroups(shuffle = True, random_state = 1, remove = ('headers', 'footers', 'quotes'))
documents = dataset.data
print('총 샘플 수 :',len(documents))

news_df = pd.DataFrame({'document' : documents})
news_df['clean_doc'] = news_df['document'].str.replace("[^a-zA-Z]", " ", regex = True)
news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x : ' '.join([w for w in x.split() if len(w) > 3]))
news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x : x.lower())

news_df.isnull().values.any()

news_df.replace("", float("NaN"), inplace = True)
news_df.isnull().values.any()

news_df.dropna(inplace = True)
print("총 샘플 수 :", len(news_df))

nltk.download('stopwords')
stop_words = stopwords.words('english')
tokenized_doc = news_df['clean_doc'].apply(lambda x : x.split())
tokenized_doc = tokenized_doc.apply(lambda x : [item for item in x if item not in stop_words])
tokenized_doc = tokenized_doc.to_list()
tokenized_doc = np.array(tokenized_doc, dtype = object)

drop_train = [index for index, sentence in enumerate(tokenized_doc) if len(sentence) <= 1]
tokenized_doc = np.delete(tokenized_doc, drop_train, axis = 0)
print('총 샘플 수 :', len(tokenized_doc))

tokenizer = Tokenizer()
tokenizer.fit_on_texts(tokenized_doc)

word2idx = tokenizer.word_index
idx2word = {value : key for key, value in word2idx.items()}
encoded = tokenizer.texts_to_sequences(tokenized_doc)

print(encoded[:2])

vocab_size = len(word2idx) + 1
print("단어 집합의 크기 :", vocab_size)

from tensorflow.keras.preprocessing.sequence import skipgrams

skip_grams = [skipgrams(sample, vocabulary_size = vocab_size, window_size = 10) for sample in encoded[:10]]

pairs, labels = skip_grams[0][0], skip_grams[0][1]
for i in range(5):
  print("({:s} ({:d}), {:s} ({:d})) -> {:d}".format(
      idx2word[pairs[i][0]], pairs[i][0],
      idx2word[pairs[i][1]], pairs[i][1],
      labels[i]))

print("전체 샘플 수:", len(skip_grams))

print(len(pairs))
print(len(labels))

skip_grams = [skipgrams(sample, vocabulary_size = vocab_size, window_size = 10) for sample in encoded]

from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Embedding, Reshape, Activation, Input
from tensorflow.keras.layers import Dot
from tensorflow.keras.utils import plot_model
from IPython.display import SVG

embedding_dim = 100

w_inputs = Input(shape = (1, ), dtype = 'int32')
word_embedding = Embedding(vocab_size, embedding_dim)(w_inputs)

c_inputs = Input(shape = (1, ), dtype = 'int32')
context_embedding = Embedding(vocab_size, embedding_dim)(c_inputs)

dot_product = Dot(axes = 2)([word_embedding, context_embedding])
dot_product = Reshape((1, ), input_shape = (1, 1))(dot_product)
output = Activation('sigmoid')(dot_product)

model = Model(inputs = [w_inputs, c_inputs], outputs = output)
model.summary()
model.compile(loss = 'binary_crossentropy', optimizer = 'adam')
plot_model(model, to_file = 'model3.png', show_shapes = True, show_layer_names = True, rankdir = 'TB')

for epoch in range(1, 6):
  loss = 0
  for _, elem in enumerate(skip_grams):
    first_elem = np.array(list(zip(*elem[0]))[0], dtype = 'int32')
    second_elem = np.array(list(zip(*elem[0]))[1], dtype = 'int32')
    labels = np.array(elem[1], dtype = 'int32')
    X = [first_elem, second_elem]
    Y = labels
    loss += model.train_on_batch(X, Y)
  print('Epoch :', epoch, 'Loss :', loss)

import gensim

f = open('vertors.txt', 'w')
f.write('{} {}\n'.format(vocab_size - 1, embedding_dim))
vectors = model.get_weights()[0]
for word, i in tokenizer.word_index.items():
  f.write('{} {}\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))
f.close()

w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary = False)

w2v.most_similar(positive = ['soldiers'])

w2v.most_similar(positive = ['doctor'])

w2v.most_similar(positive = ['police'])

w2v.most_similar(positive = ['knife'])

w2v.most_similar(positive = ['engine'])

"""##9-5 GloVe

Global Vectors for Word Representation
"""

! pip install glove_python_binary

from glove import Corpus, Glove

corpus = Corpus()

corpus.fit(result, window = 5)
glove = Glove(no_components = 100, learning_rate = 0.05)

glove.fit(corpus.matrix, epochs = 20, no_threads = 4, verbodse = True)
glove.add_dictionary(corpus.dictionary)

print(glove.most_similar("man"))

print(glove.most_similar("boy"))

print(glove.most_similar("university"))

print(glove.most_similar("water"))

print(glove.most_similar("physics"))

print(glove.most_similar("muscle"))

print(glove.most_similar("clean"))

"""##9-6 FastText"""

model.wv.most_similar("electrofishing")

from gensim.models import FastText

model = FastText(result, size = 100, window = 5, min_count = 5, workwers = 4, sg = 1)

model.wv.most_similar("electrofishing")

